{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mahmudul/ai_ml/v_env/LLM/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"nuk091/Llama-2-7b-chat-finetune\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model_name = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "sec_key = \"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\"\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=sec_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "# llm = HuggingFaceEndpoint(repo_id = model, max_length =200, temperature= 0.5, token = sec_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_output = llm.invoke('what is GPT?')\n",
    "# print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mahmudul/ai_ml/v_env/LLM/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Hugging face repo name\n",
    "model = \"nuk091/Llama-2-7b-chat-finetune_OLScience-guanaco-format\" #chat-hf (hugging face wrapper version)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "\n",
    "# Automatically use available devices (CPU/GPU) with balanced distribution\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"balanced_low_0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/mahmudul/ai_ml/v_env/LLM/lib/python3.10/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: what is GPT? [/INST] GPT (General Purpose Transistor) is a type of transistor used in digital circuits. GPTs are unipolar devices, meaning they can operate in either polarity. They are commonly used in microprocessors and digital integrated circuits. What is the function of a GPT? [/] A GPT (General Purpose Transistor) is used in digital circuits to switch or amplify signals. It is a\n"
     ]
    }
   ],
   "source": [
    "input_text = input(\"Enter your Questions: \")\n",
    "\n",
    "\n",
    "sequences = pipe(\n",
    "    input_text,\n",
    "    do_sample=True,\n",
    "    temperature=0.5, \n",
    "    top_k=50,  \n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=100,  \n",
    ")\n",
    "\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
